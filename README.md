# smart-store-awebb
In this hands-on project, I'll follow professional practices for starting a business intelligence data analytics project. 

## Overview
Calculate the total revenue generated by each customer. 
Understanding which customers contribute the most to overall sales can help businesses target their most valuable customers for loyalty programs or personalized marketing.

## Projects

### P3: Prepare Data for ETL
Building on the foundational skills developed in P1 and P2, this project focuses on preparing data for the Extract-Transform-Load (ETL) process. The goal is to ensure our input information is clean and well-organized, enabling efficient and accurate data analysis from central storage.

#### Key Objectives
1. Create a reusable `DataScrubber` class with methods for common data cleaning tasks
2. Test the `DataScrubber` class methods using Python's unittest framework
3. Use the `DataScrubber` class to clean and prepare raw data for ETL

#### Data Preparation Process
The data preparation process involves:
- Checking data consistency before and after cleaning
- Converting column data types
- Dropping unnecessary columns
- Filtering outliers
- Formatting string columns
- Handling missing data
- Removing duplicate records
- Standardizing data formats

## Required Data Columns
- CustomerID: Unique identifier for each customer.
- SaleAmount: Total revenue from each sale.

## Data Preparation Steps
- Ensure that each transaction is associated with the correct customer.
- Remove duplicates and handle missing values.
- Format string columns and standardize data.
- Perform data validation to ensure data quality.

## Project Structure
```
SMART-STORE-AWEBB/
│
├── .venv/                   # Virtual environment
├── data/
│   ├── prepared/            # Prepared, cleaned data ready for ETL
│   │   ├── customers_data_prepared.csv
│   │   ├── products_data_prepared.csv
│   │   └── sales_data_prepared.csv
│   └── raw/                 # Raw input data
│       ├── customers_data.csv
│       ├── products_data.csv
│       └── sales_data.csv
├── logs/                    # Log files
├── scripts/
│   ├── data_preparation/    # Individual data prep scripts
│   │   ├── prepare_customers_data.py
│   │   ├── prepare_products_data.py
│   │   └── prepare_sales_data.py
│   ├── data_prep.py         # Main data prep script using DataScrubber
│   └── data_scrubber.py     # Reusable DataScrubber class
├── tests/
│   └── test_data_scrubber.py # Unit tests for DataScrubber
├── utils/
│   └── logger.py            # Logging utility
└── README.md                # Project documentation
```

## Calculation Instructions
1. Group transactions by CustomerID.
2. Sum SaleAmount for each customer.

## Expected Outcome
A list of customers and the total revenue they have generated.

## Data-Driven Decision (DDDM)
KPI: Customer Lifetime Value (CLV) or Total Revenue per Customer.
- Action: Identify high-revenue customers to target them for loyalty programs, upselling, or exclusive offers, while identifying low-revenue customers who may benefit from promotional incentives.

## Setup Guide (Windows)

Run all commands from a PowerShell terminal in the root project folder.

### Step 1 - Create a Local Project Virtual Environment

```
py -m venv .venv
```

### Step 2 - Activate the Virtual Environment

```
.venv\Scripts\activate
```

### Step 3 - Install Packages

```
py -m pip install --upgrade -r requirements.txt
```

### Step 4 - Optional: Verify .venv Setup

```
py -m datafun_venv_checker.venv_checker
```

### Step 5 - Run Test for DataScrubber Class

```
py tests\test_data_scrubber.py
```

### Step 6 - Run the Data Preparation Script

```
py scripts\data_prep.py
```

## Commands Used for Data Cleaning Process

1. Clean column names by removing whitespace and converting to lowercase
   ```python
   df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')
   ```

2. Remove duplicates
   ```python
   scrubber.remove_duplicate_records()
   ```

3. Handle missing values
   ```python
   scrubber.handle_missing_data(fill_value="Unknown")
   ```

4. Format string columns
   ```python
   scrubber.format_column_strings_to_upper_and_trim('column_name')
   ```

5. Parse dates
   ```python
   scrubber.parse_dates_to_add_standard_datetime('date_column')
   ```

6. Check data consistency
   ```python
   scrubber.check_data_consistency_before_cleaning()
   scrubber.check_data_consistency_after_cleaning()
   ```

-----

## Package List

- pip
- loguru
- ipykernel
- jupyterlab
- numpy
- pandas
- matplotlib
- seaborn
- plotly
- pyspark==4.0.0.dev1
- pyspark[sql]
- git+https://github.com/denisecase/datafun-venv-checker.git#egg=datafun_venv_checker