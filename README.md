# smart-store-awebb
In this hands-on project, I'll follow professional practices for starting a business intelligence data analytics project. 

## Overview
Calculate the total revenue generated by each customer. 
Understanding which customers contribute the most to overall sales can help businesses target their most valuable customers for loyalty programs or personalized marketing.

## Projects

### P3: Prepare Data for ETL
Building on the foundational skills developed in P1 and P2, this project focuses on preparing data for the Extract-Transform-Load (ETL) process. The goal is to ensure our input information is clean and well-organized, enabling efficient and accurate data analysis from central storage.

#### Key Objectives
1. Create a reusable `DataScrubber` class with methods for common data cleaning tasks
2. Test the `DataScrubber` class methods using Python's unittest framework
3. Use the `DataScrubber` class to clean and prepare raw data for ETL

#### Data Preparation Process
The data preparation process involves:
- Checking data consistency before and after cleaning
- Converting column data types
- Dropping unnecessary columns
- Filtering outliers
- Formatting string columns
- Handling missing data
- Removing duplicate records
- Standardizing data formats

## Required Data Columns
- CustomerID: Unique identifier for each customer.
- SaleAmount: Total revenue from each sale.

## Data Preparation Steps
- Ensure that each transaction is associated with the correct customer.
- Remove duplicates and handle missing values.
- Format string columns and standardize data.
- Perform data validation to ensure data quality.

## Project Structure
```
SMART-STORE-AWEBB/
│
├── .venv/                   # Virtual environment
├── data/
│   ├── prepared/            # Prepared, cleaned data ready for ETL
│   │   ├── customers_data_prepared.csv
│   │   ├── products_data_prepared.csv
│   │   └── sales_data_prepared.csv
│   └── raw/                 # Raw input data
│       ├── customers_data.csv
│       ├── products_data.csv
│       └── sales_data.csv
├── logs/                    # Log files
├── scripts/
│   ├── data_preparation/    # Individual data prep scripts
│   │   ├── prepare_customers_data.py
│   │   ├── prepare_products_data.py
│   │   └── prepare_sales_data.py
│   ├── data_prep.py         # Main data prep script using DataScrubber
│   └── data_scrubber.py     # Reusable DataScrubber class
├── tests/
│   └── test_data_scrubber.py # Unit tests for DataScrubber
├── utils/
│   └── logger.py            # Logging utility
└── README.md                # Project documentation
```

## Calculation Instructions
1. Group transactions by CustomerID.
2. Sum SaleAmount for each customer.

## Expected Outcome
A list of customers and the total revenue they have generated.

## Data-Driven Decision (DDDM)
KPI: Customer Lifetime Value (CLV) or Total Revenue per Customer.
- Action: Identify high-revenue customers to target them for loyalty programs, upselling, or exclusive offers, while identifying low-revenue customers who may benefit from promotional incentives.

## Setup Guide (Windows)

Run all commands from a PowerShell terminal in the root project folder.

### Step 1 - Create a Local Project Virtual Environment

```
py -m venv .venv
```

### Step 2 - Activate the Virtual Environment

```
.venv\Scripts\activate
```

### Step 3 - Install Packages

```
py -m pip install --upgrade -r requirements.txt
```

### Step 4 - Optional: Verify .venv Setup

```
py -m datafun_venv_checker.venv_checker
```

### Step 5 - Run Test for DataScrubber Class

```
py tests\test_data_scrubber.py
```

### Step 6 - Run the Data Preparation Script

```
py scripts\data_prep.py
```

## Commands Used for Data Cleaning Process

1. Clean column names by removing whitespace and converting to lowercase
   ```python
   df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')
   ```

2. Remove duplicates
   ```python
   scrubber.remove_duplicate_records()
   ```

3. Handle missing values
   ```python
   scrubber.handle_missing_data(fill_value="Unknown")
   ```

4. Format string columns
   ```python
   scrubber.format_column_strings_to_upper_and_trim('column_name')
   ```

5. Parse dates
   ```python
   scrubber.parse_dates_to_add_standard_datetime('date_column')
   ```

6. Check data consistency
   ```python
   scrubber.check_data_consistency_before_cleaning()
   scrubber.check_data_consistency_after_cleaning()
   ```

-----

## Package List

- pip
- loguru
- ipykernel
- jupyterlab
- numpy
- pandas
- matplotlib
- seaborn
- plotly
- pyspark==4.0.0.dev1
- pyspark[sql]
- git+https://github.com/denisecase/datafun-venv-checker.git#egg=datafun_venv_checker

## P4: Create and Populate Data Warehouse

Building on the data preparation work done in P3, this phase focuses on designing, implementing, and populating a data warehouse for the Smart Store analysis. The data warehouse follows a star schema design to optimize for analytical queries and reporting.

### Star Schema Design

The data warehouse uses a star schema with one fact table and multiple dimension tables:

- **Fact Table**: fact_sales - Contains all sales transactions with metrics and foreign keys to dimensions
- **Dimension Tables**:
  - dim_customer - Customer information and attributes
  - dim_product - Product details and categorization
  - dim_store - Store information

This schema design supports efficient querying for business intelligence and enables us to easily answer key business questions like:
- Which customers generate the most revenue?
- Which products are top sellers?
- How do sales perform across different regions and stores?

### Implementation Steps

1. **ETL Process**:
   - Created ETL script (`scripts/etl_to_dw.py`) to extract data from prepared CSV files
   - Transformed data to conform to the star schema structure
   - Loaded data into SQLite database (`data/dw/smart_store.db`)

2. **Data Loading Results**:
   - Successfully loaded 11 customer records
   - Successfully loaded 8 product records
   - Successfully loaded 6 store records
   - Successfully loaded 94 sales transactions

3. **Verification**:
   - Validated data integrity and relationships
   - Confirmed proper loading of all dimension and fact tables
   - Ran test queries to verify ability to answer business questions

### Data Warehouse Structure

The implemented star schema includes the following tables and relationships:

#### Dimension Tables:

**dim_customer**
- customer_id (Primary Key)
- name
- region
- join_date
- loyalty_points
- customer_segment
- standard_date_time

**dim_product**
- product_id (Primary Key)
- product_name
- category
- unit_price
- stock_quantity
- subcategory

**dim_store**
- store_id (Primary Key)
- store_name

#### Fact Table:

**fact_sales**
- transaction_id (Primary Key)
- sale_date
- customer_id (Foreign Key)
- product_id (Foreign Key)
- store_id (Foreign Key)
- campaign_id
- sale_amount
- discount_percent
- payment_type

### Database Verification

The data warehouse was verified using both programmatic validation and visual inspection through VS Code's SQLite extension. 


### Business Insights

The implemented data warehouse enables us to directly answer our main business question: "What is the total revenue generated by each customer?"

A preliminary analysis reveals:
- Our top revenue-generating customers
- Revenue patterns by customer segment
- Geographic distribution of sales

This information can be used to develop targeted marketing strategies, loyalty programs, and personalized customer experiences.

## Power BI Business Intelligence Implementation

### SQL Queries and Reports
For this project, I used Power BI Desktop to connect to the SQLite database containing the Smart Store data. The key SQL query used to analyze customer revenue was:

```sql
SELECT c.customer_id, c.name, SUM(s.sale_amount) AS total_revenue
FROM fact_sales s
JOIN dim_customer c ON s.customer_id = c.customer_id
GROUP BY c.customer_id, c.name
ORDER BY total_revenue DESC
```
This query aggregates sales data to calculate total revenue by customer, allowing us to identify our most valuable customers.

### Dashboard Design Choices
The dashboard was designed with the following considerations:

- Used a bar chart for Top Customers to clearly visualize revenue differences
- Implemented date range slicers to allow filtering by time periods
- Added matrix visuals to enable cross-analysis by product categories and regions
- Created interactive drilldown capabilities from year → quarter → month
- Selected a color palette that enhances readability and highlights key insights
- Arranged visualizations to tell a cohesive story about customer purchasing patterns

### Connection Process

- Installed and configured the SQLite ODBC driver
- Created a System DSN to connect to the Smart Store database
- Connected Power BI to the database using the ODBC connection
- Loaded dimension and fact tables (dim_customer, dim_product, dim_store, fact_sales)
- Created relationships between tables in the data model
- Developed custom measures for analyzing sales performance

### Key Insights
The analysis revealed several important insights:

- Top revenue-generating customers (William White, Hermione Granger, etc.)
- Seasonal sales patterns throughout the year
- Product categories with highest revenue contribution
- Regional performance differences
![alt text](image-2.png)

## P6. BI Insights and Storytelling

### 1. The Business Goal

**Campaign Effectiveness by State Analysis**

This analysis addresses the question: "Which marketing campaigns are most effective in specific states/regions?" This is important because understanding regional preferences for marketing campaigns allows for more targeted allocation of marketing budgets and customization of campaign strategies based on regional preferences, ultimately improving ROI on marketing spend.

### 2. Data Source

I used prepared data from my data warehouse, focusing on the following tables and columns:

- **fact_sales**: transaction_id, sale_amount, campaign_id, store_id, sale_date
- **dim_campaign**: campaign_id, campaign_name
- **dim_store**: store_id, state (region)

These tables were joined to create a comprehensive view that links sales transactions to both campaigns and geographic locations.

### 3. Tools

I used Microsoft Power BI for this analysis because:

- It provides powerful visualization capabilities that support geographic analysis
- It allows for easy creation of relationships between tables
- It offers interactive filtering and slicing capabilities
- It enables the creation of custom measures for deeper analysis

### 4. Workflow & Logic

The analysis workflow included:

1. Creating relationships between fact_sales, dim_campaign, and dim_store tables
2. Creating visualizations that group sales amount by campaign and state
3. Implementing filters for product categories
4. Adding a date range selector to enable time-based analysis
5. Creating a map visualization to provide geographic context

The key OLAP techniques used were:

- **Slicing**: By product category (Clothing, Electronics, Sports)
- **Dicing**: Breaking down sales by both state and campaign simultaneously
- **Drilldown**: From total revenue to specific campaign-state combinations

### 5. Results
![alt text](image-1.png)
The analysis revealed several key insights:

1. The Spring Sale campaign is the most effective overall, generating $47,448.68 in total revenue, particularly in Missouri and Kansas.
2. Missouri has the highest total sales at $62,760.44, with Spring Sale being the most effective campaign.
3. Arkansas shows strong performance with the Back to School campaign ($11,715.82).
4. Nebraska, despite having the lowest overall sales, shows good response to the Summer Special campaign relative to its market size.
5. Non-campaign periods still generate significant revenue ($31,819.72), suggesting some regions may not require constant promotional activities.

### 6: Suggested Business Action

Based on the analysis, I recommend the following actions:

1. Increase Spring Sale marketing budget in Missouri and Kansas, where it shows the strongest performance.
2. Focus Back to School campaign resources on Arkansas, where it has proven effective.
3. Consider testing modified versions of the Summer Special in Nebraska to build on its relative success there.
4. Evaluate the necessity of campaigns in regions where non-campaign sales are strong, potentially reducing promotional discounts in those areas.
5. Develop region-specific marketing messages that align with the preferences demonstrated in each state.

### 7. Challenges

During this analysis, I encountered several challenges:

1. Limited data made it difficult to calculate more advanced metrics like a campaign effectiveness ratio.
1. Creating appropriate visualizations that clearly showed both campaign and regional performance simultaneously required several iterations.

These challenges were addressed by focusing on the available data points and creating complementary visualizations (map, table, and bar chart) that together tell a complete story about campaign effectiveness by state.