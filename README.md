# smart-store-awebb
In this hands-on project, I'll follow professional practices for starting a business intelligence data analytics project. 

## Overview
Calculate the total revenue generated by each customer. 
Understanding which customers contribute the most to overall sales can help businesses target their most valuable customers for loyalty programs or personalized marketing.

## Projects

### P3: Prepare Data for ETL
Building on the foundational skills developed in P1 and P2, this project focuses on preparing data for the Extract-Transform-Load (ETL) process. The goal is to ensure our input information is clean and well-organized, enabling efficient and accurate data analysis from central storage.

#### Key Objectives
1. Create a reusable `DataScrubber` class with methods for common data cleaning tasks
2. Test the `DataScrubber` class methods using Python's unittest framework
3. Use the `DataScrubber` class to clean and prepare raw data for ETL

#### Data Preparation Process
The data preparation process involves:
- Checking data consistency before and after cleaning
- Converting column data types
- Dropping unnecessary columns
- Filtering outliers
- Formatting string columns
- Handling missing data
- Removing duplicate records
- Standardizing data formats

## Required Data Columns
- CustomerID: Unique identifier for each customer.
- SaleAmount: Total revenue from each sale.

## Data Preparation Steps
- Ensure that each transaction is associated with the correct customer.
- Remove duplicates and handle missing values.
- Format string columns and standardize data.
- Perform data validation to ensure data quality.

## Project Structure
```
SMART-STORE-AWEBB/
│
├── .venv/                   # Virtual environment
├── data/
│   ├── prepared/            # Prepared, cleaned data ready for ETL
│   │   ├── customers_data_prepared.csv
│   │   ├── products_data_prepared.csv
│   │   └── sales_data_prepared.csv
│   └── raw/                 # Raw input data
│       ├── customers_data.csv
│       ├── products_data.csv
│       └── sales_data.csv
├── logs/                    # Log files
├── scripts/
│   ├── data_preparation/    # Individual data prep scripts
│   │   ├── prepare_customers_data.py
│   │   ├── prepare_products_data.py
│   │   └── prepare_sales_data.py
│   ├── data_prep.py         # Main data prep script using DataScrubber
│   └── data_scrubber.py     # Reusable DataScrubber class
├── tests/
│   └── test_data_scrubber.py # Unit tests for DataScrubber
├── utils/
│   └── logger.py            # Logging utility
└── README.md                # Project documentation
```

## Calculation Instructions
1. Group transactions by CustomerID.
2. Sum SaleAmount for each customer.

## Expected Outcome
A list of customers and the total revenue they have generated.

## Data-Driven Decision (DDDM)
KPI: Customer Lifetime Value (CLV) or Total Revenue per Customer.
- Action: Identify high-revenue customers to target them for loyalty programs, upselling, or exclusive offers, while identifying low-revenue customers who may benefit from promotional incentives.

## Setup Guide (Windows)

Run all commands from a PowerShell terminal in the root project folder.

### Step 1 - Create a Local Project Virtual Environment

```
py -m venv .venv
```

### Step 2 - Activate the Virtual Environment

```
.venv\Scripts\activate
```

### Step 3 - Install Packages

```
py -m pip install --upgrade -r requirements.txt
```

### Step 4 - Optional: Verify .venv Setup

```
py -m datafun_venv_checker.venv_checker
```

### Step 5 - Run Test for DataScrubber Class

```
py tests\test_data_scrubber.py
```

### Step 6 - Run the Data Preparation Script

```
py scripts\data_prep.py
```

## Commands Used for Data Cleaning Process

1. Clean column names by removing whitespace and converting to lowercase
   ```python
   df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')
   ```

2. Remove duplicates
   ```python
   scrubber.remove_duplicate_records()
   ```

3. Handle missing values
   ```python
   scrubber.handle_missing_data(fill_value="Unknown")
   ```

4. Format string columns
   ```python
   scrubber.format_column_strings_to_upper_and_trim('column_name')
   ```

5. Parse dates
   ```python
   scrubber.parse_dates_to_add_standard_datetime('date_column')
   ```

6. Check data consistency
   ```python
   scrubber.check_data_consistency_before_cleaning()
   scrubber.check_data_consistency_after_cleaning()
   ```

-----

## Package List

- pip
- loguru
- ipykernel
- jupyterlab
- numpy
- pandas
- matplotlib
- seaborn
- plotly
- pyspark==4.0.0.dev1
- pyspark[sql]
- git+https://github.com/denisecase/datafun-venv-checker.git#egg=datafun_venv_checker

## P4: Create and Populate Data Warehouse

Building on the data preparation work done in P3, this phase focuses on designing, implementing, and populating a data warehouse for the Smart Store analysis. The data warehouse follows a star schema design to optimize for analytical queries and reporting.

### Star Schema Design

The data warehouse uses a star schema with one fact table and multiple dimension tables:

- **Fact Table**: fact_sales - Contains all sales transactions with metrics and foreign keys to dimensions
- **Dimension Tables**:
  - dim_customer - Customer information and attributes
  - dim_product - Product details and categorization
  - dim_store - Store information

This schema design supports efficient querying for business intelligence and enables us to easily answer key business questions like:
- Which customers generate the most revenue?
- Which products are top sellers?
- How do sales perform across different regions and stores?

### Implementation Steps

1. **ETL Process**:
   - Created ETL script (`scripts/etl_to_dw.py`) to extract data from prepared CSV files
   - Transformed data to conform to the star schema structure
   - Loaded data into SQLite database (`data/dw/smart_store.db`)

2. **Data Loading Results**:
   - Successfully loaded 11 customer records
   - Successfully loaded 8 product records
   - Successfully loaded 6 store records
   - Successfully loaded 94 sales transactions

3. **Verification**:
   - Validated data integrity and relationships
   - Confirmed proper loading of all dimension and fact tables
   - Ran test queries to verify ability to answer business questions

### Data Warehouse Structure

The implemented star schema includes the following tables and relationships:

#### Dimension Tables:

**dim_customer**
- customer_id (Primary Key)
- name
- region
- join_date
- loyalty_points
- customer_segment
- standard_date_time

**dim_product**
- product_id (Primary Key)
- product_name
- category
- unit_price
- stock_quantity
- subcategory

**dim_store**
- store_id (Primary Key)
- store_name

#### Fact Table:

**fact_sales**
- transaction_id (Primary Key)
- sale_date
- customer_id (Foreign Key)
- product_id (Foreign Key)
- store_id (Foreign Key)
- campaign_id
- sale_amount
- discount_percent
- payment_type

### Database Verification

The data warehouse was verified using both programmatic validation and visual inspection through VS Code's SQLite extension. 


### Business Insights

The implemented data warehouse enables us to directly answer our main business question: "What is the total revenue generated by each customer?"

A preliminary analysis reveals:
- Our top revenue-generating customers
- Revenue patterns by customer segment
- Geographic distribution of sales

This information can be used to develop targeted marketing strategies, loyalty programs, and personalized customer experiences.

